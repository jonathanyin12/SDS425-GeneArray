{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import FEATURE_COLS\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/cleaned_gene_array_data.csv\")\n",
    "df_data = df[FEATURE_COLS].values\n",
    "gene_df = df[df[\"type\"] == \"gene\"]\n",
    "gene_data = gene_df[FEATURE_COLS].values\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Calculate the mean vector and covariance matrix of the reference data\n",
    "gene_mean = np.mean(gene_data, axis=0)\n",
    "gene_cov = np.cov(gene_data, rowvar=False)\n",
    "\n",
    "\n",
    "# Calculate Mahalanobis distance for each row in the dataframe\n",
    "df[\"mahalanobis_distance\"] = df.apply(\n",
    "    lambda row: distance.mahalanobis(\n",
    "        row[FEATURE_COLS], gene_mean, np.linalg.inv(gene_cov)\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Calculate the 95th percentile threshold using only the gene data\n",
    "threshold = np.percentile(\n",
    "    df[df[\"type\"] == \"gene\"][\"mahalanobis_distance\"], 100 - (100 / 3520) * 100\n",
    ")\n",
    "\n",
    "# Create a new column indicating whether each sample exceeds the threshold\n",
    "df[\"mahalanobis_outlier\"] = df[\"mahalanobis_distance\"] > threshold\n",
    "\n",
    "outlier_summary = df.groupby(\"type\")[\"mahalanobis_outlier\"].agg([\"count\", \"sum\"])\n",
    "outlier_summary[\"percentage\"] = (\n",
    "    outlier_summary[\"sum\"] / outlier_summary[\"count\"]\n",
    ") * 100\n",
    "print(outlier_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "clf = IsolationForest(random_state=0, contamination=100 / 3520).fit(gene_data)\n",
    "outlier_scores = clf.predict(df[FEATURE_COLS].values)\n",
    "\n",
    "# outlier_scores = IsolationForest(\n",
    "#     random_state=0, contamination=((176) * 2 + 100) / 3520\n",
    "# ).fit_predict(df[FEATURE_COLS].values)\n",
    "\n",
    "df[\"isolation_forest_outlier\"] = outlier_scores == -1\n",
    "\n",
    "outlier_summary = df.groupby(\"type\")[\"isolation_forest_outlier\"].agg([\"count\", \"sum\"])\n",
    "outlier_summary[\"percentage\"] = (\n",
    "    outlier_summary[\"sum\"] / outlier_summary[\"count\"]\n",
    ") * 100\n",
    "print(outlier_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "\n",
    "# Detect outliers using LocalOutlierFactor\n",
    "outlier_scores = sklearn.neighbors.LocalOutlierFactor(contamination=0.15).fit_predict(\n",
    "    df[FEATURE_COLS].values\n",
    ")\n",
    "\n",
    "df[\"lof_outlier\"] = outlier_scores == -1\n",
    "\n",
    "# Count how many samples of each type exceed the threshold\n",
    "outlier_summary = df.groupby(\"type\")[\"lof_outlier\"].agg([\"count\", \"sum\"])\n",
    "outlier_summary[\"percentage\"] = (\n",
    "    outlier_summary[\"sum\"] / outlier_summary[\"count\"]\n",
    ") * 100\n",
    "print(outlier_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_dimension_reduction_df(\n",
    "    transformed_data,\n",
    "    n_components,\n",
    "    prefix,\n",
    "    df,\n",
    "):\n",
    "    transformed_data = transformed_data[:, :n_components]\n",
    "    columns = [f\"{prefix}{i + 1}\" for i in range(n_components)]\n",
    "    result_df = pd.DataFrame(transformed_data, columns=columns)\n",
    "\n",
    "    # Add metadata columns from original DataFrame\n",
    "    for col in df.columns:\n",
    "        if col not in FEATURE_COLS:\n",
    "            result_df[col] = df[col].values\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from plotting_utils import plot_3d_scatter\n",
    "\n",
    "\n",
    "def perform_pca(X_scaled, plot_explained_variance=True):\n",
    "    \"\"\"Perform PCA and return transformed data and PCA object.\"\"\"\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    if plot_explained_variance:\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(\n",
    "            range(1, len(explained_variance_ratio) + 1),\n",
    "            cumulative_variance_ratio,\n",
    "            \"bo-\",\n",
    "        )\n",
    "        plt.xlabel(\"Number of Components\")\n",
    "        plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "        plt.title(\"PCA Explained Variance Ratio\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    return X_pca\n",
    "\n",
    "\n",
    "HOVER_COLS = [\n",
    "    \"row\",\n",
    "    \"col\",\n",
    "    \"set\",\n",
    "]\n",
    "\n",
    "X_pca = perform_pca(df_data, plot_explained_variance=True)\n",
    "\n",
    "\n",
    "pca_df = create_dimension_reduction_df(\n",
    "    X_pca,\n",
    "    3,\n",
    "    \"PC\",\n",
    "    df,\n",
    ")\n",
    "\n",
    "pca_fig = plot_3d_scatter(\n",
    "    pca_df,\n",
    "    \"PC1\",\n",
    "    \"PC2\",\n",
    "    \"PC3\",\n",
    "    title=\"3D PCA Projection\",\n",
    "    hover_data=HOVER_COLS,\n",
    "    type_col=\"type\",\n",
    "    outlier_col=\"isolation_forest_outlier\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP for dimensionality reduction\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "def perform_umap(data, n_components=3, random_state=42, n_neighbors=200, min_dist=0.8):\n",
    "    \"\"\"Perform UMAP dimensionality reduction on the input data.\"\"\"\n",
    "    umap_model = UMAP(\n",
    "        n_components=n_components,\n",
    "        random_state=random_state,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "    )\n",
    "    umap_result = umap_model.fit_transform(data)\n",
    "    return umap_result\n",
    "\n",
    "\n",
    "# Perform UMAP and create DataFrame\n",
    "X_umap = perform_umap(df_data)\n",
    "umap_df = create_dimension_reduction_df(X_umap, 3, \"UMAP\", df)\n",
    "\n",
    "# Plot UMAP results\n",
    "umap_fig = plot_3d_scatter(\n",
    "    umap_df,\n",
    "    \"UMAP1\",\n",
    "    \"UMAP2\",\n",
    "    \"UMAP3\",\n",
    "    title=\"3D UMAP Projection\",\n",
    "    type_col=\"type\",\n",
    "    hover_data=HOVER_COLS,\n",
    "    outlier_col=\"isolation_forest_outlier\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
